#cloud-config
# Whisper GPU Inference Server
# Requires: debian-12-nvidia template (NVIDIA drivers pre-installed)
# Sets up: Python environment, PyTorch with CUDA, Whisper API

hostname: whisper
manage_etc_hosts: true

package_update: true
package_upgrade: false

packages:
  - build-essential
  - python3
  - python3-pip
  - python3-venv
  - git
  - curl
  - wget
  - htop
  - ffmpeg
  - libsndfile1
  - unzip
  - avahi-daemon

users:
  - default
  - name: whisper
    groups: [sudo, video, render]
    shell: /bin/bash
    sudo: ALL=(ALL) NOPASSWD:ALL

write_files:
  # Main setup script - runs once on first boot
  - path: /opt/whisper/setup.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      # Whisper API Setup Script
      # NVIDIA drivers are pre-installed in the template

      LOG_FILE="/var/log/whisper-setup.log"
      exec > >(tee -a "$LOG_FILE") 2>&1

      echo ""
      echo "=========================================="
      echo "Whisper Setup - $(date)"
      echo "=========================================="

      SETUP_COMPLETE="/opt/whisper/.setup_complete"

      # If setup is complete, exit
      if [ -f "$SETUP_COMPLETE" ]; then
          echo "Setup already complete."
          exit 0
      fi

      # Ensure ownership
      chown -R whisper:whisper /opt/whisper

      # Verify GPU is available
      echo ""
      echo "=== Checking GPU ==="
      if ! nvidia-smi; then
          echo "ERROR: NVIDIA GPU not detected!"
          echo "Make sure you're using the debian-12-nvidia template"
          echo "and GPU passthrough is configured correctly."
          exit 1
      fi

      echo ""
      echo "GPU detected:"
      nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv

      # Setup Python environment
      echo ""
      echo "=== Setting up Python environment ==="
      sudo -u whisper /opt/whisper/setup-python.sh

      # Start service
      echo ""
      echo "=== Starting Whisper API service ==="

      chown -R whisper:whisper /opt/whisper

      systemctl daemon-reload
      systemctl enable whisper-api
      systemctl start whisper-api

      # Wait for service to start
      sleep 5

      # Verify service is running
      if systemctl is-active --quiet whisper-api; then
          touch "$SETUP_COMPLETE"
          chown whisper:whisper "$SETUP_COMPLETE"

          echo ""
          echo "=========================================="
          echo "SETUP COMPLETE!"
          echo "=========================================="
          echo ""
          echo "Whisper API: http://$(hostname -I | awk '{print $1}'):8000"
          echo "Health check: curl http://$(hostname -I | awk '{print $1}'):8000/health"
          echo ""

          # Disable setup service
          systemctl disable whisper-setup.service
      else
          echo "ERROR: whisper-api service failed to start"
          journalctl -u whisper-api -n 20 --no-pager
          exit 1
      fi

  # Systemd service for setup (runs once on first boot)
  - path: /etc/systemd/system/whisper-setup.service
    permissions: "0644"
    content: |
      [Unit]
      Description=Whisper API Setup
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      ExecStart=/opt/whisper/setup.sh
      RemainAfterExit=yes
      StandardOutput=journal+console
      StandardError=journal+console

      [Install]
      WantedBy=multi-user.target

  # Python setup script
  - path: /opt/whisper/setup-python.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      set -e

      echo "Setting up Python environment..."

      cd /opt/whisper

      # Create virtual environment
      python3 -m venv venv
      source venv/bin/activate

      # Upgrade pip
      pip install --upgrade pip wheel setuptools

      # Install PyTorch 2.0.1 with CUDA 11.8 - supports Maxwell GPUs (sm_52)
      echo "Installing PyTorch with CUDA support..."
      pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118

      # Install requirements
      pip install -r requirements.txt

      # Download the turbo model
      echo "Downloading Whisper turbo model..."
      python3 -c "
      import whisper
      print('Downloading turbo model...')
      model = whisper.load_model('turbo', device='cpu')
      print('Model downloaded successfully!')
      "

      echo "Python setup complete!"

  # Requirements file
  - path: /opt/whisper/requirements.txt
    permissions: "0644"
    content: |
      numpy<2
      openai-whisper
      fastapi>=0.109.0
      uvicorn[standard]>=0.27.0
      python-multipart>=0.0.6
      pydantic>=2.0.0

  # Whisper API service
  - path: /opt/whisper/api.py
    permissions: "0644"
    content: |
      #!/usr/bin/env python3
      """
      Whisper Transcription API
      GPU-accelerated speech-to-text using OpenAI Whisper
      """
      import os
      import tempfile
      import time
      from pathlib import Path
      from typing import Optional

      from fastapi import FastAPI, File, UploadFile, HTTPException, Query
      from pydantic import BaseModel
      import uvicorn
      import torch

      app = FastAPI(
          title="Whisper Transcription API",
          description="GPU-accelerated speech-to-text using OpenAI Whisper",
          version="2.0.0"
      )

      MODEL_NAME = os.getenv("WHISPER_MODEL", "turbo")
      DEVICE = os.getenv("WHISPER_DEVICE", "cuda" if torch.cuda.is_available() else "cpu")

      _model = None

      def get_model():
          global _model
          if _model is None:
              import whisper
              print(f"Loading Whisper model: {MODEL_NAME} on {DEVICE}")
              _model = whisper.load_model(MODEL_NAME, device=DEVICE)
              print("Model loaded successfully!")
          return _model

      class TranscriptionResponse(BaseModel):
          text: str
          language: str
          duration: Optional[float] = None
          processing_time: float
          segments: Optional[list] = None

      class HealthResponse(BaseModel):
          status: str
          model: str
          device: str
          gpu_available: bool
          gpu_name: Optional[str] = None

      @app.get("/health", response_model=HealthResponse)
      async def health_check():
          gpu_available = torch.cuda.is_available()
          gpu_name = torch.cuda.get_device_name(0) if gpu_available else None
          return HealthResponse(
              status="healthy",
              model=MODEL_NAME,
              device=DEVICE,
              gpu_available=gpu_available,
              gpu_name=gpu_name
          )

      @app.post("/transcribe", response_model=TranscriptionResponse)
      async def transcribe(
          file: UploadFile = File(...),
          language: Optional[str] = Query("pt", description="Language code (e.g., 'pt', 'en')"),
          include_segments: bool = Query(False),
          task: str = Query("transcribe"),
          temperature: float = Query(0.0, description="Sampling temperature (0 = deterministic)"),
          best_of: int = Query(5, description="Number of candidates for beam search")
      ):
          start_time = time.time()
          allowed_extensions = {".mp3", ".wav", ".m4a", ".ogg", ".flac", ".webm", ".mp4", ".opus"}
          file_ext = Path(file.filename).suffix.lower() if file.filename else ""

          if file_ext not in allowed_extensions:
              raise HTTPException(status_code=400, detail=f"Unsupported: {file_ext}")

          with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
              content = await file.read()
              tmp.write(content)
              tmp_path = tmp.name

          try:
              model = get_model()
              options = {
                  "task": task,
                  "language": language,
                  "temperature": temperature,
                  "best_of": best_of
              }

              result = model.transcribe(tmp_path, **options)

              segments_list = None
              if include_segments and "segments" in result:
                  segments_list = [
                      {"start": round(s["start"], 2), "end": round(s["end"], 2), "text": s["text"].strip()}
                      for s in result["segments"]
                  ]

              return TranscriptionResponse(
                  text=result["text"].strip(),
                  language=result.get("language", "unknown"),
                  processing_time=round(time.time() - start_time, 2),
                  segments=segments_list
              )
          except Exception as e:
              raise HTTPException(status_code=500, detail=str(e))
          finally:
              os.unlink(tmp_path)

      @app.post("/transcribe/batch")
      async def transcribe_batch(
          files: list[UploadFile] = File(...),
          language: Optional[str] = Query("pt", description="Language code (e.g., 'pt', 'en')"),
          include_segments: bool = Query(False),
          task: str = Query("transcribe"),
          temperature: float = Query(0.0, description="Sampling temperature (0 = deterministic)"),
          best_of: int = Query(5, description="Number of candidates for beam search")
      ):
          """Transcribe multiple audio files in a single request."""
          start_time = time.time()
          allowed_extensions = {".mp3", ".wav", ".m4a", ".ogg", ".flac", ".webm", ".mp4", ".opus"}
          model = get_model()

          results = []
          for file in files:
              file_start = time.time()
              file_ext = Path(file.filename).suffix.lower() if file.filename else ""

              if file_ext not in allowed_extensions:
                  results.append({
                      "filename": file.filename,
                      "error": f"Unsupported file type: {file_ext}"
                  })
                  continue

              with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
                  content = await file.read()
                  tmp.write(content)
                  tmp_path = tmp.name

              try:
                  options = {
                      "task": task,
                      "language": language,
                      "temperature": temperature,
                      "best_of": best_of
                  }

                  result = model.transcribe(tmp_path, **options)

                  segments_list = None
                  if include_segments and "segments" in result:
                      segments_list = [
                          {"start": round(s["start"], 2), "end": round(s["end"], 2), "text": s["text"].strip()}
                          for s in result["segments"]
                      ]

                  results.append({
                      "filename": file.filename,
                      "text": result["text"].strip(),
                      "language": result.get("language", "unknown"),
                      "processing_time": round(time.time() - file_start, 2),
                      "segments": segments_list
                  })
              except Exception as e:
                  results.append({
                      "filename": file.filename,
                      "error": str(e)
                  })
              finally:
                  os.unlink(tmp_path)

          return {
              "total_files": len(files),
              "total_processing_time": round(time.time() - start_time, 2),
              "results": results
          }

      if __name__ == "__main__":
          print("Pre-loading Whisper model...")
          get_model()
          print("Starting API server...")
          uvicorn.run(app, host="0.0.0.0", port=8000)

  # Systemd service for Whisper API
  - path: /etc/systemd/system/whisper-api.service
    permissions: "0644"
    content: |
      [Unit]
      Description=Whisper Transcription API
      After=network.target

      [Service]
      Type=simple
      User=whisper
      Group=whisper
      WorkingDirectory=/opt/whisper
      Environment="PATH=/opt/whisper/venv/bin:/usr/local/bin:/usr/bin:/bin"
      Environment="WHISPER_MODEL=turbo"
      Environment="WHISPER_DEVICE=cuda"
      ExecStart=/opt/whisper/venv/bin/python /opt/whisper/api.py
      Restart=always
      RestartSec=10
      SupplementaryGroups=video render

      [Install]
      WantedBy=multi-user.target

  # Promtail configuration for shipping logs to Loki
  - path: /etc/promtail/config.yaml
    permissions: "0644"
    content: |
      server:
        http_listen_port: 9080
        grpc_listen_port: 0

      positions:
        filename: /var/lib/promtail/positions.yaml

      clients:
        - url: http://192.168.20.11:31100/loki/api/v1/push

      scrape_configs:
        - job_name: whisper-api
          journal:
            max_age: 12h
            labels:
              job: whisper-api
              host: whisper-gpu
          relabel_configs:
            - source_labels: ['__journal__systemd_unit']
              regex: 'whisper-(api|setup)\.service'
              action: keep
            - source_labels: ['__journal__systemd_unit']
              target_label: unit
            - source_labels: ['__journal_priority_keyword']
              target_label: level

        - job_name: system
          journal:
            max_age: 12h
            labels:
              job: system
              host: whisper-gpu
          relabel_configs:
            - source_labels: ['__journal__systemd_unit']
              regex: '(sshd|systemd-.*|nvidia-.*)\.service'
              action: keep
            - source_labels: ['__journal__systemd_unit']
              target_label: unit
            - source_labels: ['__journal_priority_keyword']
              target_label: level

  # Systemd service for Promtail
  - path: /etc/systemd/system/promtail.service
    permissions: "0644"
    content: |
      [Unit]
      Description=Promtail Log Collector
      After=network-online.target whisper-api.service
      Wants=network-online.target

      [Service]
      Type=simple
      User=root
      ExecStart=/usr/local/bin/promtail -config.file=/etc/promtail/config.yaml
      Restart=always
      RestartSec=10

      [Install]
      WantedBy=multi-user.target

  # Promtail installation script
  - path: /opt/whisper/install-promtail.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      set -e

      PROMTAIL_VERSION="3.6.3"
      ARCH="amd64"

      echo "Installing Promtail ${PROMTAIL_VERSION}..."

      cd /tmp
      wget -q "https://github.com/grafana/loki/releases/download/v${PROMTAIL_VERSION}/promtail-linux-${ARCH}.zip"
      unzip -o "promtail-linux-${ARCH}.zip"
      chmod +x "promtail-linux-${ARCH}"
      mv "promtail-linux-${ARCH}" /usr/local/bin/promtail
      rm -f "promtail-linux-${ARCH}.zip"

      mkdir -p /var/lib/promtail
      mkdir -p /etc/promtail

      systemctl daemon-reload
      systemctl enable promtail
      systemctl start promtail

      echo "Promtail installed and started!"

runcmd:
  - mkdir -p /opt/whisper/models
  - chown -R whisper:whisper /opt/whisper
  - echo "Whisper cloud-init started at $(date)" >> /var/log/whisper-setup.log
  # Enable avahi for mDNS (whisper.local)
  - systemctl enable avahi-daemon
  - systemctl restart avahi-daemon
  # Install Promtail for log shipping
  - /opt/whisper/install-promtail.sh
  # Enable and start setup service
  - systemctl daemon-reload
  - systemctl enable whisper-setup.service
  - systemctl start whisper-setup.service
