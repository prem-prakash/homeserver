#cloud-config
# Whisper GPU Inference Server
# Requires: debian-12-nvidia template (NVIDIA drivers pre-installed)
# Sets up: Python environment, PyTorch with CUDA, Whisper API, HTTPS via Let's Encrypt

hostname: whisper
manage_etc_hosts: true

package_update: true
package_upgrade: false

packages:
  - build-essential
  - python3
  - python3-pip
  - python3-venv
  - git
  - curl
  - wget
  - htop
  - ffmpeg
  - libsndfile1
  - unzip
  - nginx
  - certbot
  - python3-certbot-dns-cloudflare

users:
  - default
  - name: whisper
    groups: [sudo, video, render]
    shell: /bin/bash
    sudo: ALL=(ALL) NOPASSWD:ALL

write_files:
  # Cloudflare credentials for Let's Encrypt
  - path: /etc/letsencrypt/cloudflare.ini
    permissions: "0600"
    content: |
      dns_cloudflare_api_token = ${cloudflare_api_token}

  # Nginx configuration
  - path: /etc/nginx/sites-available/whisper
    permissions: "0644"
    content: |
      server {
          listen 80;
          server_name ${domain};

          location / {
              return 301 https://$host$request_uri;
          }
      }

      server {
          listen 443 ssl;
          server_name ${domain};

          ssl_certificate /etc/letsencrypt/live/${domain}/fullchain.pem;
          ssl_certificate_key /etc/letsencrypt/live/${domain}/privkey.pem;
          ssl_protocols TLSv1.2 TLSv1.3;
          ssl_ciphers HIGH:!aNULL:!MD5;

          client_max_body_size 100M;

          location / {
              proxy_pass http://127.0.0.1:8000;
              proxy_set_header Host $host;
              proxy_set_header X-Real-IP $remote_addr;
              proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
              proxy_set_header X-Forwarded-Proto $scheme;

              # Extended timeouts for queued requests and long audio processing
              proxy_read_timeout 600s;
              proxy_connect_timeout 60s;
              proxy_send_timeout 600s;

              # Disable buffering for better streaming behavior
              proxy_buffering off;
              proxy_request_buffering off;
          }
      }

  # Cert renewal hook
  - path: /etc/letsencrypt/renewal-hooks/deploy/reload-nginx.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      systemctl reload nginx

  # Main setup script - runs once on first boot
  - path: /opt/whisper/setup.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      # Whisper API Setup Script
      # NVIDIA drivers are pre-installed in the template

      LOG_FILE="/var/log/whisper-setup.log"
      exec > >(tee -a "$LOG_FILE") 2>&1

      echo ""
      echo "=========================================="
      echo "Whisper Setup - $(date)"
      echo "=========================================="

      SETUP_COMPLETE="/opt/whisper/.setup_complete"

      # If setup is complete, exit
      if [ -f "$SETUP_COMPLETE" ]; then
          echo "Setup already complete."
          exit 0
      fi

      # Ensure ownership
      chown -R whisper:whisper /opt/whisper

      # Verify GPU is available
      echo ""
      echo "=== Checking GPU ==="
      if ! nvidia-smi; then
          echo "ERROR: NVIDIA GPU not detected!"
          echo "Make sure you're using the debian-12-nvidia template"
          echo "and GPU passthrough is configured correctly."
          exit 1
      fi

      echo ""
      echo "GPU detected:"
      nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv

      # Setup Python environment
      echo ""
      echo "=== Setting up Python environment ==="
      sudo -u whisper /opt/whisper/setup-python.sh

      # Start service
      echo ""
      echo "=== Starting Whisper API service ==="

      chown -R whisper:whisper /opt/whisper

      systemctl daemon-reload
      systemctl enable whisper-api
      systemctl start whisper-api

      # Wait for service to start
      sleep 5

      # Verify service is running
      if systemctl is-active --quiet whisper-api; then
          touch "$SETUP_COMPLETE"
          chown whisper:whisper "$SETUP_COMPLETE"

          echo ""
          echo "=========================================="
          echo "SETUP COMPLETE!"
          echo "=========================================="
          echo ""
          echo "Whisper API: https://${domain}"
          echo "Health check: curl https://${domain}/health"
          echo ""

          # Disable setup service
          systemctl disable whisper-setup.service
      else
          echo "ERROR: whisper-api service failed to start"
          journalctl -u whisper-api -n 20 --no-pager
          exit 1
      fi

  # Systemd service for setup (runs once on first boot)
  - path: /etc/systemd/system/whisper-setup.service
    permissions: "0644"
    content: |
      [Unit]
      Description=Whisper API Setup
      After=network-online.target
      Wants=network-online.target

      [Service]
      Type=oneshot
      ExecStart=/opt/whisper/setup.sh
      RemainAfterExit=yes
      StandardOutput=journal+console
      StandardError=journal+console

      [Install]
      WantedBy=multi-user.target

  # Python setup script
  - path: /opt/whisper/setup-python.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      set -e

      echo "Setting up Python environment..."

      cd /opt/whisper

      # Create virtual environment
      python3 -m venv venv
      source venv/bin/activate

      # Upgrade pip
      pip install --upgrade pip wheel setuptools

      # Install PyTorch 2.0.1 with CUDA 11.8 - supports Maxwell GPUs (sm_52)
      echo "Installing PyTorch with CUDA support..."
      pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118

      # Install requirements
      pip install -r requirements.txt

      # Download the turbo model
      echo "Downloading Whisper turbo model..."
      python3 -c "
      import whisper
      print('Downloading turbo model...')
      model = whisper.load_model('turbo', device='cpu')
      print('Model downloaded successfully!')
      "

      echo "Python setup complete!"

  # Requirements file
  - path: /opt/whisper/requirements.txt
    permissions: "0644"
    content: |
      numpy<2
      openai-whisper
      fastapi>=0.109.0
      uvicorn[standard]>=0.27.0
      python-multipart>=0.0.6
      pydantic>=2.0.0

  # Whisper API service
  - path: /opt/whisper/api.py
    permissions: "0644"
    content: |
      #!/usr/bin/env python3
      """
      Whisper Transcription API
      GPU-accelerated speech-to-text using OpenAI Whisper

      Features:
      - Request queuing with semaphore to process one request at a time
      - Extended timeouts for long audio files
      - Queue status endpoint for monitoring
      """
      import os
      import tempfile
      import time
      import asyncio
      from pathlib import Path
      from typing import Optional
      from datetime import datetime

      from fastapi import FastAPI, File, UploadFile, HTTPException, Query
      from pydantic import BaseModel
      import uvicorn
      import torch

      app = FastAPI(
          title="Whisper Transcription API",
          description="GPU-accelerated speech-to-text using OpenAI Whisper with request queuing",
          version="2.1.0"
      )

      MODEL_NAME = os.getenv("WHISPER_MODEL", "turbo")
      DEVICE = os.getenv("WHISPER_DEVICE", "cuda" if torch.cuda.is_available() else "cpu")

      # Semaphore to ensure only one transcription runs at a time
      # This prevents GPU memory issues and ensures stable processing
      transcription_semaphore = asyncio.Semaphore(1)

      # Queue statistics
      queue_stats = {
          "total_processed": 0,
          "total_queued": 0,
          "current_queue_size": 0,
          "last_processed_at": None,
      }

      _model = None

      def get_model():
          global _model
          if _model is None:
              import whisper
              print(f"Loading Whisper model: {MODEL_NAME} on {DEVICE}")
              _model = whisper.load_model(MODEL_NAME, device=DEVICE)
              print("Model loaded successfully!")
          return _model

      class TranscriptionResponse(BaseModel):
          text: str
          language: str
          duration: Optional[float] = None
          processing_time: float
          queue_wait_time: float = 0.0
          segments: Optional[list] = None

      class HealthResponse(BaseModel):
          status: str
          model: str
          device: str
          gpu_available: bool
          gpu_name: Optional[str] = None

      class QueueStatusResponse(BaseModel):
          current_queue_size: int
          total_processed: int
          total_queued: int
          last_processed_at: Optional[str] = None
          is_processing: bool

      @app.get("/health", response_model=HealthResponse)
      async def health_check():
          gpu_available = torch.cuda.is_available()
          gpu_name = torch.cuda.get_device_name(0) if gpu_available else None
          return HealthResponse(
              status="healthy",
              model=MODEL_NAME,
              device=DEVICE,
              gpu_available=gpu_available,
              gpu_name=gpu_name
          )

      @app.get("/queue/status", response_model=QueueStatusResponse)
      async def queue_status():
          """Get current queue status and statistics."""
          return QueueStatusResponse(
              current_queue_size=queue_stats["current_queue_size"],
              total_processed=queue_stats["total_processed"],
              total_queued=queue_stats["total_queued"],
              last_processed_at=queue_stats["last_processed_at"],
              is_processing=transcription_semaphore.locked()
          )

      async def process_transcription(tmp_path: str, options: dict, include_segments: bool) -> dict:
          """Process transcription with queue management."""
          global queue_stats

          queue_stats["total_queued"] += 1
          queue_stats["current_queue_size"] += 1
          queue_start = time.time()

          try:
              # Wait for semaphore - this ensures only one transcription at a time
              async with transcription_semaphore:
                  queue_wait_time = time.time() - queue_start
                  process_start = time.time()

                  # Run transcription in thread pool to not block event loop
                  model = get_model()
                  loop = asyncio.get_event_loop()
                  result = await loop.run_in_executor(
                      None,
                      lambda: model.transcribe(tmp_path, **options)
                  )

                  segments_list = None
                  if include_segments and "segments" in result:
                      segments_list = [
                          {"start": round(s["start"], 2), "end": round(s["end"], 2), "text": s["text"].strip()}
                          for s in result["segments"]
                      ]

                  queue_stats["total_processed"] += 1
                  queue_stats["last_processed_at"] = datetime.now().isoformat()

                  return {
                      "text": result["text"].strip(),
                      "language": result.get("language", "unknown"),
                      "processing_time": round(time.time() - process_start, 2),
                      "queue_wait_time": round(queue_wait_time, 2),
                      "segments": segments_list
                  }
          finally:
              queue_stats["current_queue_size"] -= 1

      @app.post("/transcribe", response_model=TranscriptionResponse)
      async def transcribe(
          file: UploadFile = File(...),
          language: Optional[str] = Query("pt", description="Language code (e.g., 'pt', 'en')"),
          include_segments: bool = Query(False),
          task: str = Query("transcribe"),
          temperature: float = Query(0.0, description="Sampling temperature (0 = deterministic)"),
          best_of: int = Query(5, description="Number of candidates for beam search")
      ):
          allowed_extensions = {".mp3", ".wav", ".m4a", ".ogg", ".flac", ".webm", ".mp4", ".opus"}
          file_ext = Path(file.filename).suffix.lower() if file.filename else ""

          if file_ext not in allowed_extensions:
              raise HTTPException(status_code=400, detail=f"Unsupported: {file_ext}")

          with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
              content = await file.read()
              tmp.write(content)
              tmp_path = tmp.name

          try:
              options = {
                  "task": task,
                  "language": language,
                  "temperature": temperature,
                  "best_of": best_of
              }

              result = await process_transcription(tmp_path, options, include_segments)

              return TranscriptionResponse(
                  text=result["text"],
                  language=result["language"],
                  processing_time=result["processing_time"],
                  queue_wait_time=result["queue_wait_time"],
                  segments=result["segments"]
              )
          except Exception as e:
              raise HTTPException(status_code=500, detail=str(e))
          finally:
              os.unlink(tmp_path)

      @app.post("/transcribe/batch")
      async def transcribe_batch(
          files: list[UploadFile] = File(...),
          language: Optional[str] = Query("pt", description="Language code (e.g., 'pt', 'en')"),
          include_segments: bool = Query(False),
          task: str = Query("transcribe"),
          temperature: float = Query(0.0, description="Sampling temperature (0 = deterministic)"),
          best_of: int = Query(5, description="Number of candidates for beam search")
      ):
          """Transcribe multiple audio files in a single request (processed sequentially)."""
          start_time = time.time()
          allowed_extensions = {".mp3", ".wav", ".m4a", ".ogg", ".flac", ".webm", ".mp4", ".opus"}

          results = []
          for file in files:
              file_ext = Path(file.filename).suffix.lower() if file.filename else ""

              if file_ext not in allowed_extensions:
                  results.append({
                      "filename": file.filename,
                      "error": f"Unsupported file type: {file_ext}"
                  })
                  continue

              with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
                  content = await file.read()
                  tmp.write(content)
                  tmp_path = tmp.name

              try:
                  options = {
                      "task": task,
                      "language": language,
                      "temperature": temperature,
                      "best_of": best_of
                  }

                  result = await process_transcription(tmp_path, options, include_segments)

                  results.append({
                      "filename": file.filename,
                      "text": result["text"],
                      "language": result["language"],
                      "processing_time": result["processing_time"],
                      "queue_wait_time": result["queue_wait_time"],
                      "segments": result["segments"]
                  })
              except Exception as e:
                  results.append({
                      "filename": file.filename,
                      "error": str(e)
                  })
              finally:
                  os.unlink(tmp_path)

          return {
              "total_files": len(files),
              "total_processing_time": round(time.time() - start_time, 2),
              "results": results
          }

      if __name__ == "__main__":
          print("Pre-loading Whisper model...")
          get_model()
          print("Starting API server with request queuing enabled...")
          uvicorn.run(
              app,
              host="127.0.0.1",
              port=8000,
              timeout_keep_alive=600  # 10 minute keep-alive timeout
          )

  # Systemd service for Whisper API
  - path: /etc/systemd/system/whisper-api.service
    permissions: "0644"
    content: |
      [Unit]
      Description=Whisper Transcription API
      After=network.target

      [Service]
      Type=simple
      User=whisper
      Group=whisper
      WorkingDirectory=/opt/whisper
      Environment="PATH=/opt/whisper/venv/bin:/usr/local/bin:/usr/bin:/bin"
      Environment="WHISPER_MODEL=turbo"
      Environment="WHISPER_DEVICE=cuda"
      ExecStart=/opt/whisper/venv/bin/python /opt/whisper/api.py
      Restart=always
      RestartSec=10
      SupplementaryGroups=video render

      [Install]
      WantedBy=multi-user.target

  # Promtail configuration for shipping logs to Loki
  - path: /etc/promtail/config.yaml
    permissions: "0644"
    content: |
      server:
        http_listen_port: 9080
        grpc_listen_port: 0

      positions:
        filename: /var/lib/promtail/positions.yaml

      clients:
        - url: http://192.168.20.11:31100/loki/api/v1/push

      scrape_configs:
        - job_name: whisper-api
          journal:
            max_age: 12h
            labels:
              job: whisper-api
              host: whisper-gpu
          relabel_configs:
            - source_labels: ['__journal__systemd_unit']
              regex: 'whisper-(api|setup)\.service'
              action: keep
            - source_labels: ['__journal__systemd_unit']
              target_label: unit
            - source_labels: ['__journal_priority_keyword']
              target_label: level

        - job_name: system
          journal:
            max_age: 12h
            labels:
              job: system
              host: whisper-gpu
          relabel_configs:
            - source_labels: ['__journal__systemd_unit']
              regex: '(sshd|systemd-.*|nvidia-.*)\.service'
              action: keep
            - source_labels: ['__journal__systemd_unit']
              target_label: unit
            - source_labels: ['__journal_priority_keyword']
              target_label: level

  # Systemd service for Promtail
  - path: /etc/systemd/system/promtail.service
    permissions: "0644"
    content: |
      [Unit]
      Description=Promtail Log Collector
      After=network-online.target whisper-api.service
      Wants=network-online.target

      [Service]
      Type=simple
      User=root
      ExecStart=/usr/local/bin/promtail -config.file=/etc/promtail/config.yaml
      Restart=always
      RestartSec=10

      [Install]
      WantedBy=multi-user.target

  # Promtail installation script
  - path: /opt/whisper/install-promtail.sh
    permissions: "0755"
    content: |
      #!/bin/bash
      set -e

      PROMTAIL_VERSION="3.6.3"
      ARCH="amd64"

      echo "Installing Promtail ${PROMTAIL_VERSION}..."

      cd /tmp
      wget -q "https://github.com/grafana/loki/releases/download/v${PROMTAIL_VERSION}/promtail-linux-${ARCH}.zip"
      unzip -o "promtail-linux-${ARCH}.zip"
      chmod +x "promtail-linux-${ARCH}"
      mv "promtail-linux-${ARCH}" /usr/local/bin/promtail
      rm -f "promtail-linux-${ARCH}.zip"

      mkdir -p /var/lib/promtail
      mkdir -p /etc/promtail

      systemctl daemon-reload
      systemctl enable promtail
      systemctl start promtail

      echo "Promtail installed and started!"

runcmd:
  - mkdir -p /opt/whisper/models
  - chown -R whisper:whisper /opt/whisper
  - echo "Whisper cloud-init started at $(date)" >> /var/log/whisper-setup.log

  # Create letsencrypt directories
  - mkdir -p /etc/letsencrypt/renewal-hooks/deploy

  # Get Let's Encrypt certificate
  - |
    certbot certonly \
      --dns-cloudflare \
      --dns-cloudflare-credentials /etc/letsencrypt/cloudflare.ini \
      -d ${domain} \
      --non-interactive \
      --agree-tos \
      -m ${letsencrypt_email}

  # Configure nginx
  - rm -f /etc/nginx/sites-enabled/default
  - ln -sf /etc/nginx/sites-available/whisper /etc/nginx/sites-enabled/whisper
  - systemctl enable nginx
  - systemctl start nginx

  # Install Promtail for log shipping
  - /opt/whisper/install-promtail.sh

  # Enable and start setup service
  - systemctl daemon-reload
  - systemctl enable whisper-setup.service
  - systemctl start whisper-setup.service
